```typescript
import { OpenAI } from "langchain/llms/openai";
import { BufferMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";

const model = new OpenAI({});
const memory = new BufferMemory();
const chain = new ConversationChain({
  llm: model,
  memory,
  verbose: true,
});
const res1 = await chain.call({ input: "Hi! I'm Jim." });
```

here's what's going on under the hood

```console
> Entering new chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hi there!
AI:

> Finished chain.

>> 'Hello! How are you today?'
```

Now if we run the chain again

```typescript
const res2 = await chain.call({ input: "What's my name?" });
```

we'll see that the full prompt that's passed to the model contains the input and output of our first interaction, along with our latest input

```console
> Entering new chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hi there!
AI:  Hello! How are you today?
Human: I'm doing well! Just having a conversation with an AI.
AI:

> Finished chain.

>> "Your name is Jim."
```

