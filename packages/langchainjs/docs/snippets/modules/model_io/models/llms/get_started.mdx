### Setup

To start we'll need to install the official OpenAI package:

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from "@theme/CodeBlock";

<Tabs>
  <TabItem value="npm" label="npm" default>
    <CodeBlock language="bash">npm install -S openai</CodeBlock>
  </TabItem>
  <TabItem value="yarn" label="Yarn">
    <CodeBlock language="bash">yarn add openai</CodeBlock>
  </TabItem>
  <TabItem value="pnpm" label="pnpm">
    <CodeBlock language="bash">pnpm add openai</CodeBlock>
  </TabItem>
</Tabs>

Accessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we'll want to set it as an environment variable by running:

```bash
export OPENAI_API_KEY="..."
```

If you'd prefer not to set an environment variable you can pass the key in directly via the `openAIApiKey` parameter when initializing the OpenAI LLM class:

```typescript
import { OpenAI } from "langchain/llms/openai";

const llm = new OpenAI({
  openAIApiKey: "YOUR_KEY_HERE",
});
```

otherwise you can initialize with an empty object:
```typescript
import { OpenAI } from "langchain/llms/openai";

const llm = new OpenAI({});
```

### `call`: string in -> string out
The simplest way to use an LLM is the `.call` method: pass in a string, get a string completion.

```typescript
const res = await llm.call("Tell me a joke");

console.log(res);

// "Why did the chicken cross the road?\n\nTo get to the other side."
```

### `generate`: batch calls, richer outputs
`generate` lets you can call the model with a list of strings, getting back a more complete response than just the text. This complete response can include things like multiple top responses and other LLM provider-specific information:

```typescript
const llmResult = await llm.generate(["Tell me a joke", "Tell me a poem"], ["Tell me a joke", "Tell me a poem"]);

console.log(llmResult.generations.length)

// 30

console.log(llmResult.generations[0]);

/*
  [
    {
      text: "\n\nQ: What did the fish say when it hit the wall?\nA: Dam!",
      generationInfo: { finishReason: "stop", logprobs: null }
    }
  ]
*/

console.log(llmResult.generations[1]);

/*
  [
    {
      text: "\n\nRoses are red,\nViolets are blue,\nSugar is sweet,\nAnd so are you.",
      generationInfo: { finishReason: "stop", logprobs: null }
    }
  ]
*/
```

You can also access provider specific information that is returned. This information is NOT standardized across providers.


```typescript
console.log(llmResult.llmOutput);

/*
  {
    tokenUsage: { completionTokens: 46, promptTokens: 8, totalTokens: 54 }
  }
*/

```

import AdvancedExample from "@examples/models/llm/llm_advanced.ts";

Here's an example with additional parameters, which sets `-1` for `max_tokens` to turn on token size calculations:

<CodeBlock language="typescript">{AdvancedExample}</CodeBlock>

## Advanced

_This section is for users who want a deeper technical understanding of how LangChain works. If you are just getting started, you can skip this section._

Both LLMs and Chat Models are built on top of the `BaseLanguageModel` class. This class provides a common interface for all models, and allows us to easily swap out models in chains without changing the rest of the code.

The `BaseLanguageModel` class has two abstract methods: `generatePrompt` and `getNumTokens`, which are implemented by `BaseChatModel` and `BaseLLM` respectively.

`BaseLLM` is a subclass of `BaseLanguageModel` that provides a common interface for LLMs while `BaseChatModel` is a subclass of `BaseLanguageModel` that provides a common interface for chat models.
