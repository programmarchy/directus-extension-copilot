### Setup

To start we'll need to install the official OpenAI package:

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from "@theme/CodeBlock";

<Tabs>
  <TabItem value="npm" label="npm" default>
    <CodeBlock language="bash">npm install -S openai</CodeBlock>
  </TabItem>
  <TabItem value="yarn" label="Yarn">
    <CodeBlock language="bash">yarn add openai</CodeBlock>
  </TabItem>
  <TabItem value="pnpm" label="pnpm">
    <CodeBlock language="bash">pnpm add openai</CodeBlock>
  </TabItem>
</Tabs>

Accessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we'll want to set it as an environment variable by running:

```bash
export OPENAI_API_KEY="..."
```

If you'd prefer not to set an environment variable you can pass the key in directly via the `openAIApiKey` parameter when initializing the ChatOpenAI class:

```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";

const chat = new ChatOpenAI({
  openAIApiKey: "YOUR_KEY_HERE"
});
```

otherwise you can initialize it with an empty object:

```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";

const chat = new ChatOpenAI({});
```

### Messages

The chat model interface is based around messages rather than raw text.
The types of messages currently supported in LangChain are `AIMessage`, `HumanMessage`, `SystemMessage`, `FunctionMessage`, and `ChatMessage` -- `ChatMessage` takes in an arbitrary role parameter. Most of the time, you'll just be dealing with `HumanMessage`, `AIMessage`, and `SystemMessage`

### `call`
#### Messages in -> message out

You can get chat completions by passing one or more messages to the chat model. The response will be a message.

import Example from "@examples/models/chat/chat_quick_start.ts";

<CodeBlock language="typescript">{Example}</CodeBlock>

OpenAI's chat model also supports multiple messages as input. See [here](https://platform.openai.com/docs/guides/chat/chat-vs-completions) for more information. Here is an example of sending a system and user message to the chat model:


```typescript
const response2 = await chat.call([
  new SystemMessage(
    "You are a helpful assistant that translates English to French."
  ),
  new HumanMessage("Translate: I love programming."),
]);
console.log(response2);
// AIMessage { text: "J'aime programmer." }
```

### `generate`
#### Batch calls, richer outputs

You can go one step further and generate completions for multiple sets of messages using `generate`. This returns an `LLMResult` with an additional `message` parameter.

```typescript
const response3 = await chat.generate([
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage(
      "Translate this sentence from English to French. I love programming."
    ),
  ],
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage(
      "Translate this sentence from English to French. I love artificial intelligence."
    ),
  ],
]);
console.log(response3);
/*
  {
    generations: [
      [
        {
          text: "J'aime programmer.",
          message: AIMessage { text: "J'aime programmer." },
        }
      ],
      [
        {
          text: "J'aime l'intelligence artificielle.",
          message: AIMessage { text: "J'aime l'intelligence artificielle." }
        }
      ]
    ]
  }
*/
```

You can recover things like token usage from this LLMResult:


```typescript
console.log(response3.llmOutput);
/*
  {
    tokenUsage: { completionTokens: 20, promptTokens: 69, totalTokens: 89 }
  }
*/
```
